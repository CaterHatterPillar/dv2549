\documentclass[a4paper,twocolumn,10pt]{article}

\usepackage{url}
\usepackage{float}
\usepackage{color}
\usepackage{tabto}
\usepackage{natbib}
\usepackage{listings}
\usepackage{eurosis}
\usepackage{mdwlist}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{footmisc}
\usepackage{hyperref}
\usepackage{verbatim}
\usepackage[dvipsnames]{xcolor}

\usepackage{multibib}
\newcites{web}{WEB REFERENCES}

\lstset { 
	language=C,
	tabsize=4,
	backgroundcolor=\color{black!5},
	basicstyle=\footnotesize,
}

\bibpunct[; ]{(}{)}{,}{a}{}{;}

\begin{document}
\title{PERFORMANCE~VARIATIONS~IN~EMULATED~GPGPU~KERNELS}
\author{Eric~Nilsson and Martin~Fredriksson\\
Department~of~Creative~Technologies\\
Blekinge~Institute~of~Technology\\
37179,~Karlskrona\\
Sweden\\
email: \href{mailto:EricNNilsson@gmail.com}{ericnnilsson@gmail.com} and \href{mailto:martin.fredriksson@bth.se}{martin.fredriksson@bth.se}}
\date{}

\maketitle

\thispagestyle{empty}

\keywords{GPGPU, GPU, Simulation, Emulation, DirectCompute, WARP}

% ABSTRACT
\begin{abstract}
A convenient approach toward debugging or profiling \textit{GPU}-kernels is to emulate the kernel on a \textit{CPU}. This approach is also applicable in situations where the target hardware is simply not available, as is often the case with server-side applications (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}). When performing such emulation, however, one may experience severe performance loss due to computational overhead. Consequently, the subject of this study is performance variations between three different driver types from the \textit{DirectX}-framework; using \textit{DirectCompute} and the high speed software rasterizer \textit{WARP}. As such, the performance of \textit{WARP} is compared to that of traditional \textit{GPU} hardware acceleration and the standard driver for software rasterization - the \textit{Reference~Device~Driver}. Our experimental results show a major performance boost when compared to that of software rasterization using the \textit{Reference~Device~Driver}, indicating that performance losses traditionally obstructing \textit{GPU}-kernel emulation may be sufficiently amended by technologies, such as \textit{WARP}, to the degree that such emulation can be considered viable for use in retail applications.
\end{abstract}

% INTRODUCTION
\section{INTRODUCTION}
\label{sec:introduction}
When developing \textit{GPU}-kernels, relatively small modifications in code may induce large deviations in performance due to massively parallelized instruction sets and architectural differences in-between on-chip hardware (see Performance Considerations by Kirk~\&~Hwu~\cite[ch.~6]{Kirk:2010:PMP:1841511} for an analysis on the volatility of \textit{GPGPU}-performance). The increased utilization of complex \textit{GPU}-kernels has brought forth the need of more extensive debugging- and profiling-possibilities involving access of data that may be hard to retrieve from hardware. Therefore, it may be desirable for the developer to be able to view the data being computed on the graphics card - a possibility often limited in terms of \textit{GPGPU}-technologies, possibly due to architectural differences in-between chip manufacturers.\\
A preferred solution to this problem has been to emulate such \textit{GPU}-kernels on the \textit{CPU} (see Microsoft's reference on \textit{DirectX} Driver Types~\citeweb[]{drivertypes}), such as described by Kerr~et~al.~\cite[p.~416-419]{Hwu:2011:GCG:2103614} concerning the implementation of the \textit{GPU~Ocelot} compilation framework (see GPU~Ocelot~\citeweb[]{gpuocelot}), often in exchange for substantial performance-losses. Other resons to emulate \textit{GPU}-kernels may concern pre-silicon development - that is, development for hardware not yet existant, when hardware is busy, or otherwize unavailable (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}). This study comprises an investigation into the performance of software emulation of hardware accelerated \textit{GPGPU}-kernels, by the means of analyzing several software rasterizers.\\
\\
Furthermore, this material concerns inquiry into the \textit{DirectCompute}-framework on the \textit{Windows}-platform, analyzing the performance of a \textit{GPGPU}-kernel on-chip, using the \textit{DirectX} standard software rasterizer, and utilizing the \textit{DirectX~11.1}-addition Microsoft~\textit{WARP}-technology (\textit{Windows~Advanced~Rasterization~Platform}) - which promises high-speed software emulation (see Microsoft's reference on \textit{WARP}\citeweb[]{warp}).\\
\\
This paper concludes that the performance losses inflicted by such emulation may be reduced enough for that emulation to be considered viable for retail use, as originally proposed by Microsoft (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}). Thus, this study proposes use of Microsoft~\textit{WARP}-technology in industry rasterization if graphics hardware is unavailable, not sufficient, or busy.\\
\\
As such, the study concerns the fields of simulation, emulation and \textit{GPU}-technologies - respectively, with the purpose of facilitating debugging and profiling of \textit{GPU}-kernels, whilst maintaining acceptable performance.\\
The remainder of this document presents the method and process to acquire the data used, the technologies using which it has been acquired, the results in and of their own, the conclusions based off the results and finally; the author's personal reflections surrounding future work in the area along with propositions of further study.

% CONTRIBUTION
\section{CONTRIBUTION}
\label{sec:contribution}

% CONTRIBUTION - OBJECT OF STUDY
\subsection{Object~of~Study}
\label{sec:contribution:objectofstudy}
Due to the nature of the experiment, it is important that the Object~of~Study is measurable and deterministic. For the purpose of this study, the data computed corresponds to a square matrix multiplication, since such an operation is highly parallelizable, as demonstrated by Kirk~\&~Hwu~\cite[ch.~3]{Kirk:2010:PMP:1841511}. Additionally, verifying the result of such an operation is trivial, and potential losses in computational precision may be computed from the expected data-type. The operation was therefore considered suitable for the purpose of this experiment.\\
\\
The dimension $200x200$ was selected for the respective matrices as it is big enough to execute efficiently on the \textit{GPU}, but yet reasonably sized to keep measurements collected by the means of emulation comparable to the hardware accelerated reference case.\\
Throughout this material, the compiled data will be referred to as $AB=C$, and thus make out the Object of this experiment.

% CONTRIBUTION - METHOD OF STUDY
\subsection{Method~of~Study}
\label{sec:contribution:methodofstudy}
In order to establish the Object~of~Study, $AB=C$, the experiment utilizes \textit{GPGPU}-kernels using which the result is calculated, in itâ€™s entirety, in aforementioned kernel using some target Subject (see Section~\nameref{sec:contribution:subjectsofstudy}). The experiment is devised of the following approximate steps in order to compile the Object~of~Study $AB=C$:
\begin{enumerate*}
	\item Randomize two matrices $A$ \& $B$ using desired data-type.
	\item Establish the product-matrix $AB=Ref$. The resulting matrix will be used as a reference matrix to verify the final result.
	\item Start a synchronized high-precision timer.
	\item Dispatch \textit{GPU}-kernel calculating the product matrix $AB=C$.
	\item Stop the timer once the kernel has finished execution.
	\item Establish possible deviation in-between resulting matrix $C$ and previously established matrix $Ref$.
\end{enumerate*}
The process described above make out the Method of this study.

% CONTRIBUTION - SUBJECTS OF STUDY
\subsection{Subjects~of~Study}
\label{sec:contribution:subjectsofstudy}
The \textit{GPU}-kernels described in Section~\nameref{sec:contribution:methodofstudy} are comprised of \texttt{HLSL}-syntax and are compiled \& executed using Microsoft \textit{DirectCompute}. These kernels are run using three types of acceleration technologies with varying strategies of \textit{GPU}-kernel emulation. These are comprised of the following:
\subsubsection{Hardware-Acceleration~(GPU)}
The execution of a \textit{DirectCompute}-kernel on a graphics card. This is the common case, and involves no emulation of the kernel. Thus, this case will act as a reference for the emulated Subjects.\\
During hardware acceleration on the \textit{GPU}, one may expect high performance.
\subsubsection{Software~Rasterization~(CPU)}
The emulated execution of a \textit{DirectCompute}-kernel using the \textit{DirectX} \textit{Reference~Device~Driver}. The \textit{Reference~Device~Driver} was developed for the purpose of testing and debugging, and is - although it does support some \textit{CPU}-optimizations - not intended to be used in retail applications (see Microsoft's reference on \textit{DirectX} Driver Types~\citeweb[]{drivertypes}).\\
As the \textit{Reference~Device~Driver} is designed for the purpose of accuracy, rather than speed, one may expect poor performance.

\subsubsection{Windows~Advanced~Rasterization~Platform~(CPU)}
The emulated execution of a \textit{DirectCompute}-kernel using a special software rasterizer devised by Microsoft in their latest revision of the \textit{DirectX}-framework. The driver is based off the \textit{DirectX} \textit{Reference~Device~Driver} and uses thread pooling to distribute tasks efficiently on the \textit{CPU}, along with grouping execution in batches for optimum performance. Microsoft describes \textit{WARP} as a high-performance software rasterizer, and recommends using the driver for retail applications, such as casual games (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}).\\
As there are, as of yet, few studies performed on Microsoft~\textit{WARP}; expectations are unclear, but the driver is expected to perform better than standard software rasterization.\\
\\
These three \textit{DirectX} driver-types make out the Subjects of this study.

% CONTRIBUTION - KERNELS
\subsection{Kernels}
\label{sec:contribution:kernels}
In addition to the Subject drivers mentioned in Section~\nameref{sec:contribution:subjectsofstudy}, two kernels with varying level of optimization are examined. The kernels have been implemented in accordance to the \textit{CUDA}-kernels as described by Kirk~\&~Hwu~\cite[p.~67, p.~87]{Kirk:2010:PMP:1841511}. These kernels are presented below.
\subsubsection{Matrix~mult.~w.~Thread~Blocks}
A kernel producing $AB=C$ from two given matrices, writing back $C$ for further analysis. The kernel is executed with one thread for each element in the square matrices, and likewize each produce a lone element of the resulting matrix. Execution is performed in blocks of $16x16$ threads since this was the block dimension, out of samples $8$, $16$, and $32$, that performed optimally whilst hardware accelerated on the system described in Section~\nameref{sec:contribution:equipment}.\\
This kernel will be referred to as the Basic~Kernel throughout this material.

\subsubsection{Matrix~mult.~w.~Thread~Blocks~\&~Shared~Memory}
Similar to the previous kernel, but further optimized to utilize shared memory in order to reduce time-consuming reading of global memory, as presented by Kirk~\&~Hwu~\cite[p.~77-93]{Kirk:2010:PMP:1841511}.\\
Stratton~et~al.~\cite[p.~1-3]{Stratton:2008:MEI:1485701.1485703} instructs that the \textit{CUDA}~\textit{GPGPU}-model may be applied onto multicore \textit{CPU}s, including locality-wize execution of logical thread-blocks (all threads in a block limited to a single core), with the utilization of local- and shared-memory approximately corresponding to a core's \textit{L1}-cache. Hence, the kernel is presented as a scenario due to the preconditions of \textit{WARP} - stating that a kernel optimized for \textit{GPU}-execution is likewize optimized for execution with \textit{WARP} (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}). Thus, we investigate a more optimized kernel to see whether or not this behaviour may be replicated in the experiment.\\
This kernel will be referred to as the Tiled~Kernel throughout this material.\\
\\
Furthermore, aforementioned kernels both support integer- and floating-point precision. These kernels are attached, in their entirety, under Section~\nameref{sec:appendix}.

% CONTRIBUTION - TOOLS
\subsection{Tools}
\label{sec:contribution:tools}
The experiment process has been subdivided into three major components, all of which use Microsoft~\textit{Visual~Studio~2012} for compilation. These are presented below.

\subsubsection{matrixgen}
Denotes a utility developed to generate matrices of different dimensions and data-types. Furthermore, \textit{matrixgen} compiles the reference matrix $Ref$ used when comparing the result returned from the \textit{DirectCompute}-dispatch described in the next paragraph.\\
\textit{matrixgen} is written in \texttt{C++} and utilizes \texttt{C++~AMP} to generate and multiply matrices $A~\&~B$ into product matrix $Ref$. In order to achieve random values in a \texttt{C++~AMP}-kernel the solution includes the random number generator-library \texttt{C++~AMP~RNG}. As \textit{matrixgen} utilizes Microsoft \texttt{C++~AMP}-technology, \textit{Windows~7} or later is required.

\subsubsection{experiment}
Making out the primary component of the study, \textit{experiment} uses \textit{DirectCompute} to compile the product matrix $C$ from matrices $A~\&~B$ generated by \textit{matrixgen}. The application outputs data surrounding the execution of said kernel, such as it's execution time in milliseconds, to an intermediate file.\\
\textit{experiment} is written in \texttt{C++} with it's respective \textit{DirectCompute}-kernels written in \texttt{HLSL}-syntax. As \textit{experiment} is developed using the \textit{Windows~8}~SDK, \textit{Windows~8.0} or later is required. Furthermore, \textit{experiment} requires a \textit{DirectX~11.0}- or \textit{DirectX~11.1}-compatible graphics card.

\subsubsection{analytics}
\textit{analytics} is a utility developed to compose data surrounding possible precisional deviations in-between matrices $C~\&~Ref$. \textit{analytics} compiles the minimum- and maximum-deviation encountered, as well as to calculate the standard deviation of said precisional deviation. In turn, analytics outputs this information to an intermediate file.\\
\textit{analytics} is written in \texttt{C++}.\\
\\
These three applications are, in turn, run as subprocesses in a script specifying the various configurations and number of times to run each program. This script, written in \texttt{Python}, then compiles the assorted results of these applications and outputs a range of files suitably formatted for interpretation by \textit{Gnuplot}.\\
The source code manufactured for the sake of this study is freely available via an online \textit{Git} repository~\citeweb[]{github}, along with a guide on how to compile and run the solution in order to replicate the experiment. Furthermore, the complete results collected and used throughout this study are also available for download, and may be acquired for further analysis.

% CONTRIBUTION - EQUIPMENT
\subsection{Equipment}
\label{sec:contribution:equipment}
The results presented in this study is gathered from experiments performed on a system with the following specifications:
\begin{description*}
	\item[CPU]	Intel~Q9550~Quad~Core~$2.83$GHz
	\item[GPU]	ATI~Radeon~HD~5800
	\item[OS]	\tabto{0.35cm}\textit{Windows~8.0} % hack
\end{description*}
This system setup was selected for use, for the purpose of this study, as Microsoft claims that the \textit{WARP} driver performs best on modern quad-core \textit{CPU}s (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}).

% CONTRIBUTION - PROCESS OF STUDY
\subsection{Process~of~Study}
\label{sec:contribution:processofstudy}
For the purpose of this study, the Object~of~Study - being $200x200$ matrices - were randomized with numbers in-between zero and ten. The product matrix of these matrices was then computed $100$ times for each configuration. In this way, the Basic~Kernel was run with each Subject~of~Study - being Hardware~Acceleration, Software~Rasterization and \textit{WARP} - respectively, likewize as with the Tiled~Kernel. For each execution, data surrounding the dispatch-time of each kernel (meaning the time taken to execute corresponding kernel, regardless of program initialization) was garnered along with precision-wize deviational data.\\
The process described above was then repeated for integer- and floating point-precision.\\
\\
The measurements gathered from these executions make out the Results presented in this study.

% CONTRIBUTION - RESULTS
\subsection{Results}
\label{sec:contribution:results}
Based off the average of the collected execution times described in Section~\nameref{sec:contribution:processofstudy}; results indicate an improvement in the performance of the kernels when using Microsoft~\textit{WARP}, compared to the performance of the \textit{DirectX}~\textit{Reference~Device~Driver} from which \textit{WARP} is derived. Table~\ref{tab:contribution:results:summaryint} demonstrates a performance gain with the Hardware~Accelerated Subject when utilizing shared memory amongst blocks; with varying results for the other Subjects - either increasing or decreasing execution time (see Table~\ref{tab:contribution:results:summaryfloat}).

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r|r|r|}
	\cline{2-3}
							& \multicolumn{1}{|c|}{\textbf{BASIC}} & \multicolumn{1}{|c|}{\textbf{TILED}}	\\ \hline
	\multicolumn{1}{|l|}{\textbf{HARD}}	& $1.16$			& $0.24$ 	& $-79.3\%$  					\\ \hline
	\multicolumn{1}{|l|}{\textbf{SOFT}}	& $11610.02$		& $9866.40$	& $-15.0\%$   					\\ \hline
	\multicolumn{1}{|l|}{\textbf{WARP}}	& $15.31$			& $18.97$	& $+23.9\%$   					\\ \hline
\end{tabular}
\end{center}
\caption{Average execution time in milliseconds of a $200x200$ integer matrix multiplication.}
\label{tab:contribution:results:summaryint}
\end{table}

\begin{table}[hbt]
\begin{center}
\begin{tabular}{r|r|r|r|}
	\cline{2-3}
							& \multicolumn{1}{|c|}{\textbf{BASIC}} & \multicolumn{1}{|c|}{\textbf{TILED}}	\\ \hline
	\multicolumn{1}{|l|}{\textbf{HARD}}	& $0.77$			& $0.22$		& $-71.4\%$  				\\ \hline
	\multicolumn{1}{|l|}{\textbf{SOFT}}	& $10247.03$		& $10909.88$	& $+6.5\%$   				\\ \hline
	\multicolumn{1}{|l|}{\textbf{WARP}}	& $14.08$			& $17.44$		& $+23.9\%$  				\\ \hline
\end{tabular}
\end{center}
\caption{Average execution time in milliseconds of a $200x200$ floating point matrix multiplication.}
\label{tab:contribution:results:summaryfloat}
\end{table}

Using both integer- and floating point-precision, the performance of \textit{WARP} is impaired by the kernel utilizing shared memory according to the data presented in Tables~\ref{tab:contribution:results:summaryint}~\&~\ref{tab:contribution:results:summaryfloat}. In Microsoft's guide on \textit{WARP} (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}), the author claims that an application, if tuned to run efficiently on hardware, will run efficiently whilst emulated using \textit{WARP} - and vice versa. However, the data collected for this study rather indicates an increase in execution time of $23.9$\%, independant of precision, even though the same kernel accelerates the hardware accelerated Subject by roughly $70$\%. The floating point scenario of this effect is visualized in Figure~\ref{fig:contribution:results:warp:msswarp}.

\begin{figure}[htb]
\begin{center}
	\resizebox{ \columnwidth }{!}{\input{../../msswarp}}
	\caption{\textit{WARP} execution time with floating point-precision for the Basic- and Tiled-kernel, with visualized mean and standard deviations. Values outside of their respective standard deviations have been clipped for the sake of clarity.}
	\label{fig:contribution:results:warp:msswarp}
\end{center}
\end{figure}

Integer precision calculation showed no sign of precisonal loss whatsoever. Meanwhile, all floating point-experiments experienced an equal average loss of computational precision. However, the data collected indicates no divergence in the precisional loss of respective Subject. The average deviations in precision experienced equally with each configuration, in relation to the $Ref$-matrix descibed in Section~\nameref{sec:contribution:methodofstudy}, are presented in Table~\ref{tab:contribution:results:avgprecision}.
\label{sec:contribution:results:computationalprecision}
\begin{table}[hbt]
\begin{center}
	\begin{tabular}{|r|r|r|}
		\hline
		\textbf{Minimum} 	& \textbf{Maximum} 	& \textbf{Standard} 	\\ \hline
		0.0   			& ~0.01  			& ~0.0025      		\\ \hline
	\end{tabular}
\caption {Average precisional deviations in floating~point-operations for all Subjects.}
\label{tab:contribution:results:avgprecision}
\end{center}
\end{table}

These results are expected as \textit{WARP} conforms to the precision requirements of the \textit{Direct3D~10}- and \textit{10.1}-specification (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}). See Microsoft's documentation on Floating-point Rules~\citeweb[]{floatingpointrules} for more information surrounding floating point-precision in the \textit{Direct3D}-framework.

% CONCLUSION
\section{CONCLUSION}
\label{sec:conclusion}
Based on the results presented in Section~\nameref{sec:contribution:results}, using \textit{WARP} to emulate the kernels presented in this study has magnitudes greater performance than if one were to apply the \textit{DirectX}~\textit{Reference~Device~Driver} in the same manner. Hence, if one were to compare the execution of \textit{WARP} \& the \textit{Reference~Device~Driver} side-by-side, and assume the same area of application, \textit{WARP} is superior in terms of execution time - assuming the same preconditions as those presented in this material.\\
However, keeping in mind the major performance improvements offered by Microsoft~\textit{WARP}, it is important to consider that the two may be appropriate for different purposes. The \textit{DirectX}~\textit{Reference~Device~Driver} is primarily proposed by Microsoft as a debugging-/pre-silicon-development tool, whereas \textit{WARP} is intended for use in a broader sense (see Microsoft's reference on \textit{WARP}~\citeweb[]{warp}) - such as to render graphics for casual games - in addition to debugging and error-profiling purposes. This calls for further inquiry into what the flaws of using Microsoft~\textit{WARP} may be.\\
\\
In conclusion; this study proposes, pursuant to the established performance of Microsoft~\textit{WARP}, that \textit{WARP}-like technologies are feasable for extended use in applications - for purposes other than debugging and profiling.

% CONCLUSION - FUTURE WORK
\subsection{Future~Work}
\label{sec:conclusion:futurework}
For the sake of brevity, the author suggests complementary elaboration into double precision calculations in \textit{DirectCompute}-kernels, with the intent of examining whether or not the Subjects detailed in Section~\nameref{sec:contribution:subjectsofstudy} may have differentiating effects on computational precision.\\
\\
Additionally, the use of more complex kernels in coagency with the \textit{WARP}-driver should be examined in order to study the effects of more demanding emulation on the \textit{CPU}.

\newpage

% BIBLIOGRAPHY
\bibliographystyle{eurosis}
\bibliographystyleweb{eurosis}

\bibliography{eurosis}
\bibliographyweb{eurosis}

\newpage

% APPENDIX
\section{APPENDIX}
\label{sec:appendix}

% APPENDIX - MATRIX MULT. W. BLOCKS
\subsection*{Matrix~mult.~w.~Blocks}
\begin{lstlisting}
#ifndef DV2549_FXS_MULTFLOATBASIC_FX
#define DV2549_FXS_MULTFLOATBASIC_FX

#include <CommonFloat.fx>

[ numthreads( BLOCK_SIZE, BLOCK_SIZE, 1 ) ]
void main(
	uint3 tIdx : SV_GroupThreadID,
	uint3 bIdx : SV_GroupID ) {
	const uint row = bIdx.y*BLOCK_SIZE+tIdx.y;
	const uint col = bIdx.x*BLOCK_SIZE+tIdx.x;
	if( row>=cRows || col>=cCols ) {
		return;
	}
    
	float sum = 0;
	for( uint i = 0; i<aRows; i++ ) {
		uint idxA = row*aRows+i;
		uint idxB = col+bRows*i;
		sum += mA[idxA]*mB[idxB];
	}
	mC[row*cRows+col] = sum;
}

#endif // DV2549_FXS_MULTFLOATBASIC_FX
\end{lstlisting}

\newpage
\vspace*{0.08cm} % hack

% APPENDIX - MATRIX MULT. W. BLOCKS & SHARED MEMORY
\subsection*{Matrix~mult.~w.~Blocks~\&~Shared~Memory}
\begin{lstlisting}
#ifndef DV2549_FXS_MULTFLOATTILE_H
#define DV2549_FXS_MULTFLOATTILE_H

#include <CommonFloat.fx>

groupshared float mAs[BLOCK_SIZE][BLOCK_SIZE];
groupshared float mBs[BLOCK_SIZE][BLOCK_SIZE];

[ numthreads( BLOCK_SIZE, BLOCK_SIZE, 1 ) ]
void main(
	uint3 tIdx : SV_GroupThreadID,
	uint3 bIdx : SV_GroupID ) {
	const uint row = bIdx.y*BLOCK_SIZE+tIdx.y;
	const uint col = bIdx.x*BLOCK_SIZE+tIdx.x;
	
	float sum = 0;
	const uint blocks = ceil( 
		(float)aRows/(float)BLOCK_SIZE );
	for( uint i = 0; i<blocks; i++ ) {
		mAs[tIdx.y][tIdx.x] = mA[ 
			row*aRows+( i*BLOCK_SIZE+tIdx.x )];
		mBs[tIdx.y][tIdx.x] = mB[ 
			col+bRows*( i*BLOCK_SIZE+tIdx.y )];
		GroupMemoryBarrierWithGroupSync();

		for( uint j = 0; j<BLOCK_SIZE; j++ ) {
			sum += 
				mAs[tIdx.y][j]*mBs[j][tIdx.x];
		}
		GroupMemoryBarrierWithGroupSync();
	}
	if( row>=cRows || col>=cCols ) {
		return;
	}
	mC[row*cRows+col] = sum;
}

#endif // DV2549_FXS_MULTFLOATTILE_H

\end{lstlisting}

\end{document}

